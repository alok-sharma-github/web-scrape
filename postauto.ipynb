{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to login to LinkedIn\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    time.sleep(2)\n",
    "   \n",
    "    email_input = driver.find_element(By.ID, 'username')\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "   \n",
    "    email_input.send_keys(email)\n",
    "    password_input.send_keys(password)\n",
    "   \n",
    "    login_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "    login_button.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    # Handle the quick verification step\n",
    "    handle_verification(driver)\n",
    "\n",
    "def handle_verification(driver):\n",
    "    # Check if the verification page is present\n",
    "    try:\n",
    "        verification_message = driver.find_element(By.XPATH, '//button[@aria-label=\"descriptionVerify\"]')\n",
    "        if verification_message:\n",
    "            print(\"Please complete the verification step manually...\")\n",
    "            input(\"Press Enter after you have completed the verification.\")\n",
    "            time.sleep(5)  # Wait a bit for the page to refresh after verification\n",
    "    except Exception as e:\n",
    "        print(f\"No verification step detected or error: {e}\")\n",
    "\n",
    "# Function to perform a simple search and scroll\n",
    "def search_simple(driver, query, max_scroll=5):\n",
    "    search_url = f'https://www.linkedin.com/search/results/all/?keywords={query}'\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    for _ in range(max_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "\n",
    "# Function to extract post content using BeautifulSoup\n",
    "def extract_post_content(soup):\n",
    "    post_contents = []\n",
    "    try:\n",
    "        containers = soup.find_all('div', {'class': 'update-components-text relative update-components-update-v2__commentary'})\n",
    "        \n",
    "        for container in containers:\n",
    "            spans = container.find_all('span', {'dir': 'ltr'})\n",
    "            post_content = ' '.join(span.get_text(separator=' ', strip=True) for span in spans)\n",
    "            post_contents.append(post_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting post content: {e}\")\n",
    "\n",
    "    return post_contents\n",
    "\n",
    "# Function to extract URLs and post content from search results\n",
    "def extract_urls_and_content(driver):\n",
    "    urls_and_content = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        containers = soup.find_all('div', {'id': 'fie-impression-container'})\n",
    "        \n",
    "        for container in containers:\n",
    "            link_tag = container.find('a', {'class': 'app-aware-link update-components-actor__meta-link'})\n",
    "            if link_tag and 'href' in link_tag.attrs:\n",
    "                url = link_tag['href']\n",
    "                post_content = extract_post_content(soup)\n",
    "                urls_and_content.append((url, post_content))\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting URLs and content: {e}\")\n",
    "\n",
    "    return urls_and_content\n",
    "\n",
    "# Function to save data to CSV\n",
    "def save_to_csv(data, filename='scraped_data_3.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['URL', 'Post Content'])\n",
    "        for url, post_content in data:\n",
    "            writer.writerow([url, post_content])\n",
    "\n",
    "# Function to go to the next page of search results and scroll\n",
    "def go_to_next_page(driver, max_scroll=5):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//button[contains(@class, \"artdeco-pagination__button--next\")]')\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # wait for the next page to load\n",
    "        for _ in range(max_scroll):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error going to the next page: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function\n",
    "def main():\n",
    "    email = 'patel844732sunny@gmail.com'\n",
    "    password = 'sunny844732A'\n",
    "    query = input(\"Enter the search query: \")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    try:\n",
    "        login_to_linkedin(driver, email, password)\n",
    "        search_simple(driver, query)\n",
    "        \n",
    "        all_urls_and_content = []\n",
    "        while True:\n",
    "            urls_and_content = extract_urls_and_content(driver)\n",
    "            all_urls_and_content.extend(urls_and_content)\n",
    "            if not go_to_next_page(driver):\n",
    "                break\n",
    "        \n",
    "        save_to_csv(all_urls_and_content)\n",
    "        print(f\"Data scraped and saved to scraped_data.csv\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No verification step detected or error: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[@aria-label=\"descriptionVerify\"]\"}\n",
      "  (Session info: chrome=126.0.6478.127); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00E6C203+27395]\n",
      "\t(No symbol) [0x00E03E04]\n",
      "\t(No symbol) [0x00D01B7F]\n",
      "\t(No symbol) [0x00D42C65]\n",
      "\t(No symbol) [0x00D42D3B]\n",
      "\t(No symbol) [0x00D7EC82]\n",
      "\t(No symbol) [0x00D639E4]\n",
      "\t(No symbol) [0x00D7CB24]\n",
      "\t(No symbol) [0x00D63736]\n",
      "\t(No symbol) [0x00D37541]\n",
      "\t(No symbol) [0x00D380BD]\n",
      "\tGetHandleVerifier [0x01123AB3+2876339]\n",
      "\tGetHandleVerifier [0x01177F7D+3221629]\n",
      "\tGetHandleVerifier [0x00EED674+556916]\n",
      "\tGetHandleVerifier [0x00EF478C+585868]\n",
      "\t(No symbol) [0x00E0CE44]\n",
      "\t(No symbol) [0x00E09858]\n",
      "\t(No symbol) [0x00E099F7]\n",
      "\t(No symbol) [0x00DFBF4E]\n",
      "\tBaseThreadInitThunk [0x763F5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77DABD2B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DABCB1+561]\n",
      "\n",
      "Error going to the next page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=126.0.6478.127)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00E6C203+27395]\n",
      "\t(No symbol) [0x00E03E04]\n",
      "\t(No symbol) [0x00D01B7F]\n",
      "\t(No symbol) [0x00CDE483]\n",
      "\t(No symbol) [0x00D6A06F]\n",
      "\t(No symbol) [0x00D7C3D6]\n",
      "\t(No symbol) [0x00D63736]\n",
      "\t(No symbol) [0x00D37541]\n",
      "\t(No symbol) [0x00D380BD]\n",
      "\tGetHandleVerifier [0x01123AB3+2876339]\n",
      "\tGetHandleVerifier [0x01177F7D+3221629]\n",
      "\tGetHandleVerifier [0x00EED674+556916]\n",
      "\tGetHandleVerifier [0x00EF478C+585868]\n",
      "\t(No symbol) [0x00E0CE44]\n",
      "\t(No symbol) [0x00E09858]\n",
      "\t(No symbol) [0x00E099F7]\n",
      "\t(No symbol) [0x00DFBF4E]\n",
      "\tBaseThreadInitThunk [0x763F5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77DABD2B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DABCB1+561]\n",
      "\n",
      "Data scraped and saved to scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
