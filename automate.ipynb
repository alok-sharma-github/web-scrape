{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to login to LinkedIn\n",
    "def login_to_linkedin(driver, email, password):\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    time.sleep(2)\n",
    "   \n",
    "    email_input = driver.find_element(By.ID, 'username')\n",
    "    password_input = driver.find_element(By.ID, 'password')\n",
    "   \n",
    "    email_input.send_keys(email)\n",
    "    password_input.send_keys(password)\n",
    "   \n",
    "    login_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "    login_button.click()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for a hashtag and scroll\n",
    "def search_hashtag(driver, hashtag, max_scroll=5):\n",
    "    search_url = f'https://www.linkedin.com/feed/hashtag/{hashtag}/'\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)\n",
    "   \n",
    "    for _ in range(max_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform a simple search and scroll\n",
    "def search_simple(driver, query, max_scroll=5):\n",
    "    search_url = f'https://www.linkedin.com/search/results/all/?keywords={query}'\n",
    "    driver.get(search_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    for _ in range(max_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to scrape the first post under the hashtag\n",
    "# def scrape_first_post(driver):\n",
    "#     post_text = \"\"\n",
    "#     try:\n",
    "#         first_post = driver.find_element(By.CLASS_NAME, 'occludable-update')\n",
    "#         post_text = first_post.find_element(By.CLASS_NAME, 'feed-shared-update-v2__description').text\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error finding or extracting the first post: {e}\")\n",
    "   \n",
    "#     return post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract URLs from search results\n",
    "def extract_urls(driver):\n",
    "    urls = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        containers = soup.find_all('div', {'id': 'fie-impression-container'})\n",
    "        \n",
    "        for container in containers:\n",
    "            link_tag = container.find('a', {'class': 'app-aware-link update-components-actor__meta-link'})\n",
    "            if link_tag and 'href' in link_tag.attrs:\n",
    "                urls.append(link_tag['href'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting URLs: {e}\")\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(url, post, filename='two_scraped_data.csv'):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['url', 'post'])  # Write the header\n",
    "        writer.writerow([url, post])     # Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to go to the next page of search results and scroll\n",
    "def go_to_next_page(driver, max_scroll=5):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//button[contains(@class, \"artdeco-pagination__button--next\")]')\n",
    "        next_button.click()\n",
    "        time.sleep(5)  # wait for the next page to load\n",
    "        for _ in range(max_scroll):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error going to the next page: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated function to scrape post content\n",
    "def scrape_post_content(driver):\n",
    "    post_elements = driver.find_elements(By.XPATH, '//div[@class=\"update-components-text relative update-components-update-v2__commentary\"]/div[@class=\"break-words tvm-parent-container\"]/span[@dir=\"ltr\"]')\n",
    "    posts = []\n",
    "    for element in post_elements:\n",
    "        posts.append(element.text)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    email = 'kingofkingsaxox@proton.me'\n",
    "    password = 'Alok@123'\n",
    "    query = input(\"Enter the search query: \")\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    try:\n",
    "        login_to_linkedin(driver, email, password)\n",
    "        search_simple(driver, query)\n",
    "        \n",
    "        all_urls = []\n",
    "        while True:\n",
    "            urls = extract_urls(driver)\n",
    "            all_urls.extend(urls)\n",
    "            posts = scrape_post_content(driver)\n",
    "            \n",
    "            if not go_to_next_page(driver):\n",
    "                break\n",
    "        \n",
    "        save_to_csv(all_urls,posts)\n",
    "        print(f\"Data scraped and saved to scraped_data.csv\")\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error going to the next page: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=126.0.6478.127)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00E6C203+27395]\n",
      "\t(No symbol) [0x00E03E04]\n",
      "\t(No symbol) [0x00D01B7F]\n",
      "\t(No symbol) [0x00CDE483]\n",
      "\t(No symbol) [0x00D6A06F]\n",
      "\t(No symbol) [0x00D7C3D6]\n",
      "\t(No symbol) [0x00D63736]\n",
      "\t(No symbol) [0x00D37541]\n",
      "\t(No symbol) [0x00D380BD]\n",
      "\tGetHandleVerifier [0x01123AB3+2876339]\n",
      "\tGetHandleVerifier [0x01177F7D+3221629]\n",
      "\tGetHandleVerifier [0x00EED674+556916]\n",
      "\tGetHandleVerifier [0x00EF478C+585868]\n",
      "\t(No symbol) [0x00E0CE44]\n",
      "\t(No symbol) [0x00E09858]\n",
      "\t(No symbol) [0x00E099F7]\n",
      "\t(No symbol) [0x00DFBF4E]\n",
      "\tBaseThreadInitThunk [0x763F5D49+25]\n",
      "\tRtlInitializeExceptionChain [0x77DABD2B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77DABCB1+561]\n",
      "\n",
      "Data scraped and saved to scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
